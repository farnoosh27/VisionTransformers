## CNN vs Transformers
Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions (resource: Transformers in Vision: A Survey).
## Why would transformers be used instead of CNN at all?

Transformers excel at capturing long-range dependencies, which is crucial for understanding relationships between distant elements in sequences. This capability is valuable for recognizing connections between distant pixels or regions in images. Additionally, Transformers offer scalability to handle images of various resolutions without major architectural changes. They also show promise in few-shot learning scenarios, enabling models to generalize to new concepts with limited training data, a significant advantage in computer vision with sparse labeled datasets.



## links
[Evaluating the fairness of computer vision models](https://ai.meta.com/blog/dinov2-facet-computer-vision-fairness-evaluation/?utm_source=linkedin&utm_medium=organic_social&utm_campaign=blog&utm_content=video)
